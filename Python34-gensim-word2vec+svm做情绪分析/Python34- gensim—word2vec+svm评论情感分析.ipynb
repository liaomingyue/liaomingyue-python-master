{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gensim—word2vec+svm评论情感分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "G:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using Theano backend.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "#用gensim去做word2vec数据处理，使用sklearn做svm建模\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.svm import SVC\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "熊高雄买了一个手机，他说这个手机用了两三天就坏了！  negative\n"
     ]
    }
   ],
   "source": [
    "#载入数据，做数据预处理（分词），切分训练集与测试集\n",
    "def load_file_and_preprocessing():\n",
    "    neg=pd.read_excel('data/neg.xls',header=None,index=None)\n",
    "    pos=pd.read_excel('data/pos.xls',header=None,index=None)\n",
    "\n",
    "    cw = lambda x: list(jieba.cut(x))\n",
    "\n",
    "    # 新增一列 word ,存放分好词的评论，pos[0]代表表格第一列\n",
    "\n",
    "    pos['words'] = pos[0].apply(cw)\n",
    "    neg['words'] = neg[0].apply(cw)\n",
    "\n",
    "    # np.ones(len(pos)) 新建一个长度为len(pos)的数组并初始化元素全为1来标注好评\n",
    "    # np.concatenate（）连接数组\n",
    "    # axis=0 向下执行方法 axis=1向右执行方法\n",
    "    y = np.concatenate((np.ones(len(pos)), np.zeros(len(neg))),axis=0)\n",
    "\n",
    "    # train_test_split：从样本中随机的按比例选取train data和testdata\n",
    "    # 一般形式：train_test_split(train_data,train_target,test_size=0.4, random_state=0)\n",
    "    # train_data：所要划分的样本特征集\n",
    "    # train_target：所要划分的样本结果（标注）\n",
    "    # test_size：样本占比，如果是整数的话就是样本的数量\n",
    "    # random_state：是随机数的种子。\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(np.concatenate((pos['words'], neg['words'])), y, test_size=0.2)\n",
    "    \n",
    "    np.save('svm_data/y_train.npy',y_train)\n",
    "    np.save('svm_data/y_test.npy',y_test)\n",
    "    return x_train,x_test\n",
    "\n",
    "\n",
    "\n",
    "# 对每个句子的所有词向量取均值，来生成一个句子的vector\n",
    "def build_sentence_vector(text, size,imdb_w2v):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in text:\n",
    "        try:\n",
    "            vec += imdb_w2v[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "# 计算词向量\n",
    "def get_train_vecs(x_train, x_test):\n",
    "    n_dim = 300\n",
    "    # 初始化模型和词表\n",
    "    imdb_w2v = Word2Vec(x_train, size=n_dim, min_count=10)\n",
    "    # imdb_w2v = Word2Vec(size=300, window=5, min_count=10, workers=12)\n",
    "    # imdb_w2v.build_vocab(x_train)\n",
    "    #\n",
    "    # imdb_w2v.train(x_train,\n",
    "    #                total_examples=imdb_w2v.corpus_count,\n",
    "    #                epochs=imdb_w2v.iter)\n",
    "    \n",
    "\n",
    "    train_vecs = np.concatenate([build_sentence_vector(z, n_dim, imdb_w2v) for z in x_train])\n",
    "    # train_vecs = scale(train_vecs)\n",
    "\n",
    "    np.save('svm_data/train_vecs.npy', train_vecs)\n",
    "    print(train_vecs.shape) \n",
    "    # 在测试集上训练\n",
    "    imdb_w2v.train(x_test,total_examples=imdb_w2v.corpus_count,total_words=len(x_train),epochs=imdb_w2v.iter)\n",
    "    # imdb_w2v.train(x_test,\n",
    "    #                total_examples=imdb_w2v.corpus_count,\n",
    "    #                epochs=imdb_w2v.iter)\n",
    "\n",
    "    imdb_w2v.save('svm_data/w2v_model/w2v_model.pkl')\n",
    "    # Build test tweet vectors then scale\n",
    "    test_vecs = np.concatenate([build_sentence_vector(z, n_dim, imdb_w2v) for z in x_test])\n",
    "    # test_vecs = scale(test_vecs)\n",
    "    np.save('svm_data/test_vecs.npy', test_vecs)\n",
    "    print(test_vecs.shape) \n",
    "\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    train_vecs=np.load('svm_data/train_vecs.npy')\n",
    "    y_train=np.load('svm_data/y_train.npy')\n",
    "    test_vecs=np.load('svm_data/test_vecs.npy')\n",
    "    y_test=np.load('svm_data/y_test.npy')\n",
    "    return train_vecs,y_train,test_vecs,y_test\n",
    "\n",
    "# 训练svm模型\n",
    "\n",
    "def svm_train(train_vecs,y_train,test_vecs,y_test):\n",
    "    clf=SVC(kernel='rbf',verbose=True)\n",
    "    clf.fit(train_vecs,y_train)\n",
    "    joblib.dump(clf, 'svm_data/svm_model/model.pkl')\n",
    "    print(clf.score(test_vecs,y_test)) \n",
    "\n",
    "\n",
    "# 构建待预测句子的向量\n",
    "\n",
    "def get_predict_vecs(words):\n",
    "    n_dim = 300\n",
    "    imdb_w2v = Word2Vec.load('svm_data/w2v_model/w2v_model.pkl')\n",
    "    #imdb_w2v.train(words)\n",
    "    train_vecs = build_sentence_vector(words, n_dim,imdb_w2v)\n",
    "    #print train_vecs.shape\n",
    "    return train_vecs\n",
    "\n",
    "# 对单个句子进行情感判断\n",
    "\n",
    "def svm_predict(string):\n",
    "    words=jieba.lcut(string)\n",
    "    words_vecs=get_predict_vecs(words)\n",
    "    clf=joblib.load('svm_data/svm_model/model.pkl')\n",
    "    result=clf.predict(words_vecs)\n",
    "    \n",
    "    if int(result[0])==1:\n",
    "        print(string,' positive') \n",
    "    else:\n",
    "        print(string,' negative') \n",
    "\n",
    "#\n",
    "# x_train,x_test = load_file_and_preprocessing()\n",
    "# get_train_vecs(x_train,x_test)\n",
    "# train_vecs,y_train,test_vecs,y_test = get_data()\n",
    "# svm_train(train_vecs,y_train,test_vecs,y_test)\n",
    "\n",
    "#对输入句子情感进行判断\n",
    "# string='电池充完了电连手机都打不开.简直烂的要命.真是金玉其外,败絮其中!连5号电池都不如'\n",
    "# string='这手机真棒，从1米高的地方摔下去就坏了'\n",
    "# string=\"我今天买了一台电脑，刚用一会电池就没电了，建议大家不要买\"\n",
    "# string=\"这个手机很好了，续航时间特别长\"\n",
    "# string=\"我的电脑花了7000元，配置很好，但是用了几天坏了，很热，不便捷\"\n",
    "# string=\"本来拍了几张图片分享下的，京东把晒单驳回了！一个差评就这样做！唉\"\n",
    "# string=\"手机发热厉害，昨天直接充不进电了！！！我为啥要买这个*破*手机啊？\"\n",
    "string=\"手机反应速度快，价格便宜，照相也很清楚，玩游戏也不卡，外观漂亮，物美价廉，！\"\n",
    "string=\"熊高雄买了一个手机，他说这个手机用了两三天就坏了！\"\n",
    "svm_predict(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
